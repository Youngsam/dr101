{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시간차 학습 (Temporal-difference Learning)\n",
    "\n",
    "* 시간차 학습(TD learning) 역시 MC와 마찬가지로 환경에 대한 완전한 정보를 요구하지 않고 경험 데이터로부터 직접 학습한다.\n",
    "* 하지만 시간차 학습은 MC와는 달리 한 에피소드가 끝나기를 기다렸다 상태 값들을 업데이트할 필요가 없다.\n",
    "* 왜냐하면 시간차 학습은 자신의 다음 상태에 대한 예측값을 현재 가치함수를 이용하여 획득해 활용하기 때문이다 (bootstrapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가치 함수들\n",
    "\n",
    "<img src=\"./val_fs.PNG\" width=\"600\" height=\"400\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD-error 와 TD-target\n",
    "\n",
    "<img src=\"./td_err.PNG\" width=600 height=400 align=left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD 학습을 이용한  policy evalution\n",
    "\n",
    "## 1. Driving Home 예제: MC vs TD\n",
    "<img src=\"driving.PNG\" width=700 height=400>\n",
    "<br/><br/>\n",
    "<img src=\"driving2.PNG\" width=700 height=400>\n",
    "\n",
    "## 2. 랜덤 워크 예제\n",
    "<img src=\"random_walk.PNG\" width=700 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from racetrack_env import RacetrackEnv, Map, REWARD_SUCCESS\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_episode(states):\n",
    "    episode = []\n",
    "    k = len(states)//2 # start node index\n",
    "    try:\n",
    "        while states[k] not in terminals:\n",
    "            episode.append(states[k])\n",
    "            action = -1 if random.randint(0, 1) else 1\n",
    "            k += action\n",
    "    finally:\n",
    "        episode.append(states[k])\n",
    "    return episode\n",
    "\n",
    "def gen_episodes(states, trials):\n",
    "    episodes = []\n",
    "    for n in range(trials):\n",
    "        episodes.append(gen_episode(states))\n",
    "    return episodes\n",
    "\n",
    "def initialize(states):\n",
    "    vals_dic = {}\n",
    "    for state in states:\n",
    "        if state not in terminals:\n",
    "            vals_dic[state]=0.5\n",
    "        else:\n",
    "            vals_dic[state]=0\n",
    "    return vals_dic\n",
    "\n",
    "def value_update(eps, vals_dic, alpha):\n",
    "    for t in range(0, len(eps)-1):\n",
    "        new_val = vals_dic[eps[t]]+alpha*(reward(eps[t+1])+vals_dic[eps[t+1]]-vals_dic[eps[t]])\n",
    "        vals_dic[eps[t]]=new_val\n",
    "\n",
    "def reward(state):\n",
    "    if state == 'rterm':\n",
    "        return 1.\n",
    "    else:\n",
    "        return 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['lterm', 'A', 'B', 'C', 'D', 'E', 'rterm']\n",
    "terminals = ('lterm', 'rterm')\n",
    "values_dic1 = initialize(states)\n",
    "values_dic10 = initialize(states)\n",
    "values_dic100 = initialize(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes1 = gen_episodes(states, 1)\n",
    "episodes10 = gen_episodes(states, 10)\n",
    "episodes100 = gen_episodes(states, 100)\n",
    "\n",
    "for episode in episodes1:\n",
    "    value_update(episode, values_dic1, 0.1)\n",
    "for episode in episodes10:\n",
    "    value_update(episode, values_dic10, 0.1)\n",
    "for episode in episodes100:\n",
    "    value_update(episode, values_dic100, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([1/6., 2/6., 3/6., 4/6., 5/6.], 'ko-')\n",
    "plt.plot([values_dic1[x] for x in states[1:-1]], 'ko-', color='r', label='1')\n",
    "plt.plot([values_dic10[x] for x in states[1:-1]], 'ko-', color='g', label='10')\n",
    "plt.plot([values_dic100[x] for x in states[1:-1]], 'ko-', color='b', label='100')\n",
    "plt.xticks(range(6), states[1:-1])\n",
    "plt.ylim([0,1])\n",
    "plt.xlim([-1, 6])\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## TD 학습을 이용한 Control 문제\n",
    "<br/>\n",
    "<img src=\"sarsa.PNG\", width=700 height=400>\n",
    "<br/><br/>\n",
    "<img src=\"q_learning.PNG\", width=700 height=400>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racetrack problem\n",
    "<br/><br/>\n",
    "<img src=\"./race.PNG\" width=700 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODE = 10000  # Recommend: 10000 for E-Greedy\n",
    "MAX_STEP = 70\n",
    "EGREEDY_EPS = 0.1\n",
    "GAMMA = 0.99\n",
    "ALPHA = 0.1\n",
    "SHOW_TERM = 5001\n",
    "SPOLICY = \"EGreedy\"\n",
    "SAVE_FILENM = \"Racetrack_sarsa{}.sav\".format(SPOLICY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    with open('racetrack_map_4.txt', 'r') as f:\n",
    "        amap = Map(f.read())\n",
    "\n",
    "    vel_info = (\n",
    "        0, 3,  # vx min / max\n",
    "        -3, 3   # vy min / max\n",
    "    )\n",
    "    env = RacetrackEnv(amap, vel_info, MAX_STEP)\n",
    "    return env\n",
    "\n",
    "def egreedy_policy(env, Q, state, e_no, test_action=None):\n",
    "    aprobs = Q[state]\n",
    "    if test_action is not None:\n",
    "        action = test_action\n",
    "    else:\n",
    "        action = np.random.choice(np.flatnonzero(aprobs == aprobs.max()))\n",
    "    nA = env.action_space.n\n",
    "    eps = EGREEDY_EPS * (1 - float(e_no) / MAX_EPISODE)\n",
    "    # eps = EGREEDY_EPS\n",
    "    A = np.ones(nA) * eps / nA\n",
    "    A[action] += (1.0 - eps)\n",
    "    return A\n",
    "\n",
    "def egreedy_action(aprobs, nA):\n",
    "    return np.random.choice(range(nA), p=aprobs)\n",
    "\n",
    "def test_egreedy_policy():\n",
    "    env = make_env()\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    best_action = 1\n",
    "    state = None\n",
    "    aprobs = egreedy_policy(env, Q, state, 1, best_action)\n",
    "    assert np.array_equal(aprobs, np.array([0.02, 0.92, 0.02, 0.02, 0.02]))\n",
    "    n = 0\n",
    "    acnt = defaultdict(int)\n",
    "    TRY_CNT = 100\n",
    "    while n < TRY_CNT:\n",
    "        action = np.random.choice(range(nA), p=aprobs)\n",
    "        acnt[action] += 1\n",
    "        n += 1\n",
    "    EPS_CNT = 100 * EGREEDY_EPS\n",
    "    assert TRY_CNT - acnt[best_action] < 2 * EPS_CNT\n",
    "\n",
    "def make_greedy_policy(Q):\n",
    "    def func(state):\n",
    "        A = np.zeros_like(Q[state], dtype=float)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] = 1.0\n",
    "        return A\n",
    "    return func\n",
    "\n",
    "def _run_step(env, Q, N, state, action, nA, n_episode, n_step):\n",
    "    nstate, reward, done, _ = env.step(action)\n",
    "\n",
    "    naprobs = egreedy_policy(env, Q, nstate, n_episode + 1)\n",
    "    naction = np.random.choice(range(nA), p=naprobs)  # SARSA\n",
    "\n",
    "    v = Q[state][action]\n",
    "    nv = Q[nstate][naction]\n",
    "    td_target = reward + GAMMA * nv\n",
    "    td_delta = td_target - v\n",
    "    Q[state][action] += ALPHA * td_delta\n",
    "    return nstate, naction, reward, done  \n",
    "\n",
    "def _print_policy_progress(Q, state, N, action):\n",
    "    print(\"  \", state, Q[state], action)\n",
    "\n",
    "def _print_done_msg(reward):\n",
    "    if reward == REWARD_SUCCESS:\n",
    "        print(\"   SUCCESS!!\")\n",
    "    else:\n",
    "        print(\"   DONE\")\n",
    "\n",
    "def learn_Q(env):\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    N = defaultdict(lambda: np.ones(nA))\n",
    "    rewards_list = []\n",
    "\n",
    "    for n_episode in range(MAX_EPISODE):\n",
    "        state = env.reset()\n",
    "        naprobs = egreedy_policy(env, Q, state, n_episode + 1)\n",
    "        action = np.random.choice(range(nA), p=naprobs)\n",
    "\n",
    "        show = (n_episode + 1) % SHOW_TERM == 0\n",
    "        if show:\n",
    "            print(\"========== Policy: {}, Episode: {} / {} ==========\".\n",
    "                 format(SPOLICY, n_episode + 1, MAX_EPISODE))\n",
    "\n",
    "        for n_step in range(MAX_STEP):\n",
    "            state, action, reward, done = _run_step(env, Q, N, state, action, nA,\n",
    "                                                    n_episode, n_step)\n",
    "            if reward == REWARD_SUCCESS:\n",
    "                rewards_list.append(1.)\n",
    "            if show:\n",
    "                _print_policy_progress(Q, state, N, action)\n",
    "            if done:\n",
    "                if show:\n",
    "                    _print_done_msg(reward)\n",
    "                break\n",
    "\n",
    "    print(\"The average of rewards: {}\".format(np.sum(rewards_list)/MAX_EPISODE))\n",
    "    #print(np.sum(rewards_list), len(rewards_list))\n",
    "    return Q\n",
    "\n",
    "def run():\n",
    "    env = make_env()\n",
    "    Q = learn_Q(env)\n",
    "    play_policy = make_greedy_policy(Q)\n",
    "    env.play(play_policy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def _run_step(env, Q, N, state, action, nA, n_episode, n_step):\n",
    "    nstate, reward, done, _ = env.step(action)\n",
    "\n",
    "    naprobs = egreedy_policy(env, Q, nstate, n_episode + 1)\n",
    "    naction = np.random.choice(range(nA), p=naprobs)  # SARSA\n",
    "\n",
    "    v = Q[state][action]\n",
    "    nv = Q[nstate][naction]\n",
    "    td_target = reward + GAMMA * nv\n",
    "    td_delta = td_target - v\n",
    "    Q[state][action] += ALPHA * td_delta\n",
    "    return nstate, naction, reward, done  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_step(env, Q, N, state, nA, n_episode, n_step):\n",
    "    aprobs = egreedy_policy(env, Q, state, n_episode)\n",
    "    action = np.random.choice(range(nA), p=aprobs)\n",
    "    \n",
    "    nstate, reward, done, _ = env.step(action)\n",
    "    naction = np.argmax(Q[nstate])  # Q-learning\n",
    "\n",
    "    v = Q[state][action]\n",
    "    nv = Q[nstate][naction]\n",
    "    td_target = reward + GAMMA * nv\n",
    "    td_delta = td_target - v\n",
    "    Q[state][action] += ALPHA * td_delta\n",
    "    return nstate, naction, reward, done  \n",
    "\n",
    "def learn_Q(env):\n",
    "    nA = env.action_space.n\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    N = defaultdict(lambda: np.ones(nA))\n",
    "    rewards_list = []\n",
    "\n",
    "    for n_episode in range(MAX_EPISODE):\n",
    "        state = env.reset()\n",
    "        naprobs = egreedy_policy(env, Q, state, n_episode + 1)\n",
    "        action = np.random.choice(range(nA), p=naprobs)\n",
    "\n",
    "        show = (n_episode + 1) % SHOW_TERM == 0\n",
    "        if show:\n",
    "            print(\"========== Policy: {}, Episode: {} / {} ==========\".\n",
    "                 format(SPOLICY, n_episode + 1, MAX_EPISODE))\n",
    "\n",
    "        for n_step in range(MAX_STEP):\n",
    "            state, action, reward, done = _run_step(env, Q, N, state, nA,\n",
    "                                                    n_episode, n_step)\n",
    "            if reward == REWARD_SUCCESS:\n",
    "                rewards_list.append(1.)\n",
    "            if show:\n",
    "                _print_policy_progress(Q, state, N, action)\n",
    "            if done:\n",
    "                if show:\n",
    "                    _print_done_msg(reward)\n",
    "                break\n",
    "\n",
    "    print(\"The average of rewards: {}\".format(np.sum(rewards_list)/MAX_EPISODE))\n",
    "    #print(np.sum(rewards_list), len(rewards_list))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
