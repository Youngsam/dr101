{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 몬테 카를로 방법 \n",
    "\n",
    "* 엎서 우리는 다이내믹 프로그래밍으로 policy evaluation을 하는 것을 보았다.\n",
    "* 하지만 Gridworld 예제처럼 MDP 환경에 대한 정보를 우리가 완전히 알지 못하는 경우가 많다.\n",
    "* 몬테 카를로 방법은 이런 문제에 대해 시뮬레이션을 통해 얻은 샘플들을 이용해 MDP 문제를 해결한다.\n",
    "<br/>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://mblogthumb-phinf.pstatic.net/MjAxNzA4MTJfMjMg/MDAxNTAyNDgwMTY2ODU4.XlSmxGGdqffwf-fmiJLh8WW1fbyUN9Yzari7vsRAgoIg.Azdu31iXYs3XzjvpnCiXdtudxofdyEVqQaaZ5isk2I0g.JPEG.unlvsu2010/1501-Melb-Casino-CasinoGames-BlackJack-Table-974x676-02-2_copy.jpg?type=w800\" width=\"600\" height=\"400\">\n",
    "</p>\n",
    "<br/>\n",
    "\n",
    "## 몬테카를로 Policy Evaluation\n",
    "* 어떤 policy를 따라 얻은 데이터들을 에피소드 단위로 얻는다.\n",
    "* MC-학습에 따른 가치 함수 정의\n",
    "    * MC-학습에서는 policy evaluation을 샘플에 대한 평균을 통해 수행한다.\n",
    "    * $v(s_t)=V(s_t)+\\alpha(G_t-v(s_t)) \\\\\n",
    "    \\text{where}\\: G_t=R_{t+1}+\\gamma R_{t+2}+\\cdots =\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$\n",
    "\n",
    "##  MDP 환경 고려\n",
    "* 만일 MDP 환경이 stationary 하다면 다음이 낫다.\n",
    "    * $N(S_t)=N(S_t)+1$\n",
    "    * $v(S_t)=v(S_t)+\\frac{1}{N(S_t)}(G_t-v(S_t))$\n",
    "* 만일 환경이 non-stationary 하다면 항상 일정한 비율(학습률)로 누적평균을 구하는 것이 더 낫다.\n",
    "    * $v(S_t)=v(S_t)+\\alpha(G_t-v(S_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Exploring Starts 알고리즘\n",
    "\n",
    "<br/>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://t1.daumcdn.net/cfile/tistory/99F6D4335C6F8BAC29\">\n",
    "</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 블랙잭 게임 예시\n",
    "* [실제 게임 체험](http://math.hws.edu/eck/cs271/js-work/Blackjack.html)\n",
    "* 환경의 상태 수: 200개\n",
    "    * 현재 플레이어 패의 수: 12-21 (12 미만은 무조건 더 받는다는 설정)\n",
    "    * 딜러가 보여주는 패의 수: ACE~10\n",
    "    * 플레이어에게 'usable' ACE가 있는지의 여부: Yes or No\n",
    "* 행동\n",
    "     * Stick: 카드를 더 받지 않고 끝낸다.\n",
    "     * Hit: 카드를 더 받는다.\n",
    "* 보상의 경우들\n",
    "     * +1 : 플레이어의 패 합이 딜러보다 크다면\n",
    "     * 0 : 플레이어 카드와 딜러 카드의 패 합이 같다면\n",
    "     * -1 : 딜러 카드의 패 합이 플레이어보다 더 높다면\n",
    "     * -1 : 플레이어 카드 패 합이 21을 넘는다면\n",
    "     * 0 : 그 외의 경우들\n",
    "* [MC policy evaluation](./gym_blackjack_exp.ipynb)\n",
    "* [MC contorl](./gym_blackjack_hw.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
