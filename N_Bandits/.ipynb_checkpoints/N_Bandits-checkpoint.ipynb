{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색을 학습에 활용하는 방법: 엡실론-그리디 방법\n",
    "\n",
    "<img src=\"https://ichef.bbci.co.uk/news/660/media/images/74580000/jpg/_74580127_162317629(1).jpg\" width=600 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enemy:\n",
    "    def __init__(self, kawi, bawi, bo):\n",
    "        assert sum([kawi, bawi, bo]) == 1.0\n",
    "        self.hands = {0:'kawi', 1:'bawi', 2:'bo'}\n",
    "        self.prob = [kawi, bawi, bo]\n",
    "    \n",
    "    def draw(self):\n",
    "        action =  np.random.choice(3, p=self.prob)\n",
    "        return self.hands[action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enmy = Enemy(0.5, 0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enmy.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [enmy.draw() for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(cnt)), cnt.values())\n",
    "plt.xticks(range(len(cnt)), cnt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, epsilon):\n",
    "        self.hands = {0:'kawi', 1:'bawi', 2:'bo'}\n",
    "        self.eps = epsilon\n",
    "        self.count = {'kawi':[0, 0, 0], 'bawi':[0, 0, 0], 'bo': [0, 0, 0]}  # hand: [N_draw, N_win, N_lose]\n",
    "        \n",
    "    def draw(self):\n",
    "        decision = 'exploit' if np.random.rand() > self.eps else 'explore'\n",
    "        if decision == 'exploit':\n",
    "            return self.greedy_act()\n",
    "        elif decision == 'explore':\n",
    "            return self.random_act()\n",
    "        \n",
    "    def greedy_act(self):\n",
    "        temp_summary = {}\n",
    "        for hand, vals in self.count.items():\n",
    "            if sum(vals) == 0:\n",
    "                temp_summary[hand] = 0\n",
    "            else:\n",
    "                win_rate = vals[1]/sum(vals)\n",
    "                temp_summary[hand] = win_rate\n",
    "        return max(temp_summary, key=temp_summary.get)\n",
    "    \n",
    "    def random_act(self):\n",
    "        return self.hands[np.random.choice(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, enemy, agent):\n",
    "        self.enemy = enemy\n",
    "        self.agent = agent\n",
    "        \n",
    "    def one_play(self):\n",
    "        enemy_hand = self.enemy.draw()\n",
    "        myhand = self.agent.draw()\n",
    "        out = None\n",
    "        if myhand == enemy_hand:\n",
    "            out = 0\n",
    "        elif myhand=='kawi' and enemy_hand=='bo':\n",
    "            out = 1\n",
    "        elif myhand=='bawi' and enemy_hand=='kawi':\n",
    "            out = 1\n",
    "        elif myhand=='bo' and enemy_hand=='bawi':\n",
    "            out = 1\n",
    "        else:\n",
    "            out = -1\n",
    "        return out, myhand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enemy = Enemy(0.9, 0.05, 0.05)\n",
    "agent = Agent(0.1)\n",
    "game = Game(enemy, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_avg = []\n",
    "results = []\n",
    "temp = []\n",
    "for n in range(100):\n",
    "    temp = []\n",
    "    for k in range(20):\n",
    "        out, agt_hand = game.one_play()\n",
    "        if k == 1:\n",
    "            agent.count[agt_hand][out] += 1\n",
    "        temp.append(out)\n",
    "    results.append(np.mean(temp))\n",
    "    run_avg.append(np.mean(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(run_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [agent.draw() for _ in range(1000)]\n",
    "cnt = Counter(samples)\n",
    "plt.bar(range(len(cnt)), cnt.values())\n",
    "plt.xticks(range(len(cnt)), cnt.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(params, agent_eps, n_trails=100):\n",
    "    enemy = Enemy(params[0], params[1], params[2])\n",
    "    agent = Agent(agent_eps)\n",
    "    game = Game(enemy, agent)\n",
    "    run_avg = []\n",
    "    results = []\n",
    "    temp = []\n",
    "    for n in range(n_trails):\n",
    "        temp = []\n",
    "        for k in range(20):\n",
    "            out, agt_hand = game.one_play()\n",
    "            if k == 1:\n",
    "                agent.count[agt_hand][out] += 1\n",
    "            temp.append(out)\n",
    "        results.append(np.mean(temp))\n",
    "        run_avg.append(np.mean(results))\n",
    "    return run_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [0.9, 0.05, 0.05]\n",
    "eps_01 = experiment(params, agent_eps=0.1)\n",
    "eps_03 = experiment(params, agent_eps=0.3)\n",
    "eps_005 = experiment(params, agent_eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(eps_01, color='r', label='EPS=0.1')\n",
    "plt.plot(eps_03, color='g', label='EPS=0.3')\n",
    "plt.plot(eps_005, color='b', label='EPS=0.05')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 잘 터지는 슬롯 머신 고르기 문제\n",
    "<br/>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/82/Las_Vegas_slot_machines.jpg\" alt=\"Drawing\" style=\"width: 600px; height:400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = [0.2, 0.8, 1.5, 0.5, 1.3, 0.5, -0.2, -1.0, 0.9, 0.4]\n",
    "mus = {'q'+str(i+1):mus[i] for i in range(len(mus))}\n",
    "sigmas = {'q'+str(i+1):1 for i in range(len(mus))}\n",
    "data = {'q'+str(i+1):[] for i in range(len(mus))}\n",
    "for i in range(len(mus)):\n",
    "    for _ in range(1000):\n",
    "        key = 'q'+str(i+1)\n",
    "        data[key].append(np.random.normal(mus[key], sigmas[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.DataFrame(data)\n",
    "data_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 보상값의 분산이 큰 경우, 탐색적 활동이 탐욕적 활동보다 더 필요하다.\n",
    "2. 설사 보상값의 출현패턴이 결정적이라고 하더라도, 환경이 비고정적이라면 탐색적 활동은 여전히 가치있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DobakAgent:\n",
    "    def __init__(self, epsilon):\n",
    "        ks = ['q'+str(i+1) for i in range(10)]\n",
    "        self.N = {k:0 for k in ks}\n",
    "        self.Q = {k:0 for k in ks}\n",
    "        self.eps = epsilon\n",
    "        \n",
    "    def decide(self):\n",
    "        decision = 'exploit' if np.random.rand() > self.eps else 'explore'\n",
    "        if decision == 'exploit':\n",
    "            return self.greedy_act()\n",
    "        elif decision == 'explore':\n",
    "            return self.random_act()\n",
    "        \n",
    "    def greedy_act(self):\n",
    "        return max(self.Q, key=self.Q.get)\n",
    "    \n",
    "    def random_act(self):\n",
    "        n = np.random.choice(10)\n",
    "        return list(self.Q.keys())[n]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__init__(self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(agt_00, agt_01, agt_001, sig=1, steps=500, n_runs=1000):\n",
    "    res_agt_00 = {}\n",
    "    res_agt_01 = {}\n",
    "    res_agt_001 = {}\n",
    "    for n in range(n_runs):\n",
    "        # 한 번의 런마다 하나의 문제를 생성한다.\n",
    "        mus = np.random.uniform(-3, 3, 10)\n",
    "        mus = {'q'+str(i+1):mus[i] for i in range(len(mus))}\n",
    "        sigmas = {'q'+str(i+1):sig for i in range(len(mus))}\n",
    "        res_agt_00[n] = []\n",
    "        res_agt_01[n] = []\n",
    "        res_agt_001[n] = []\n",
    "        # 에이전트들을 리셋 시킨다.\n",
    "        agt_00.reset()\n",
    "        agt_01.reset()\n",
    "        agt_001.reset()\n",
    "        for step in range(steps):\n",
    "            # 각 에이전트는 슬롯머신 손잡이를 선택한다.\n",
    "            action_00 = agt_00.decide()\n",
    "            action_01 = agt_01.decide()\n",
    "            action_001 = agt_001.decide()\n",
    "            # 각 에이전트의 선택에 따른 결과를 받는다.\n",
    "            reward_00 = np.random.normal(mus[action_00], sigmas[action_00])\n",
    "            reward_01 = np.random.normal(mus[action_01], sigmas[action_01])\n",
    "            reward_001 = np.random.normal(mus[action_001], sigmas[action_001])\n",
    "            res_agt_00[n].append(reward_00)\n",
    "            res_agt_01[n].append(reward_01)\n",
    "            res_agt_001[n].append(reward_001)\n",
    "            # 각 에이전트의 카운터를 올리고, 평균 보상값을 업데이트한다.\n",
    "            agt_00.N[action_00] += 1\n",
    "            agt_01.N[action_01] += 1\n",
    "            agt_001.N[action_001] += 1\n",
    "            agt_00.Q[action_00] += (1/agt_00.N[action_00]) * (reward_00 - agt_00.Q[action_00])\n",
    "            agt_01.Q[action_01] += (1/agt_01.N[action_01]) * (reward_01 - agt_01.Q[action_01])\n",
    "            agt_001.Q[action_001] += (1/agt_001.N[action_001]) * (reward_001 - agt_001.Q[action_001])\n",
    "    # 각 타입 스탭마다 2000번의 시행결과를 평균낸다.\n",
    "    res_00, res_01, res_001 = [], [], []\n",
    "    for step in range(steps):\n",
    "        temp_00, temp_01, temp_001 = [], [], []\n",
    "        for n in range(n_runs):\n",
    "            temp_00.append(res_agt_00[n][step])\n",
    "            temp_01.append(res_agt_01[n][step])\n",
    "            temp_001.append(res_agt_001[n][step])\n",
    "        res_00.append(np.mean(temp_00))\n",
    "        res_01.append(np.mean(temp_01))\n",
    "        res_001.append(np.mean(temp_001))\n",
    "    return res_00, res_01, res_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt_00 = DobakAgent(0.0)\n",
    "agt_01 = DobakAgent(0.1)\n",
    "agt_001 = DobakAgent(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_agt_00, res_agt_01, res_agt_001 = experiment(agt_00, agt_01, agt_001)\n",
    "\n",
    "plt.plot(res_agt_00, color='r', label='Greedy')\n",
    "plt.plot(res_agt_01, color='g', label='EPS=0.1')\n",
    "plt.plot(res_agt_001, color='b', label='EPS=0.01')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분산이 심한 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = {'q'+str(i+1):2 for i in range(len(mus))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'q'+str(i+1):[] for i in range(len(mus))}\n",
    "for i in range(len(mus)):\n",
    "    for _ in range(1000):\n",
    "        key = 'q'+str(i+1)\n",
    "        data[key].append(np.random.normal(mus[key], sigmas[key]))\n",
    "data_pd = pd.DataFrame(data)\n",
    "sns.violinplot(data=data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt_00 = DobakAgent(0.0)\n",
    "agt_01 = DobakAgent(0.1)\n",
    "agt_001 = DobakAgent(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cavg_agt_00, cavg_agt_01, cavg_agt_001 = experiment(agt_00, agt_01, agt_001, sig=2)\n",
    "\n",
    "plt.plot(cavg_agt_00, color='r', label='Greedy')\n",
    "plt.plot(cavg_agt_01, color='g', label='EPS=0.1')\n",
    "plt.plot(cavg_agt_001, color='b', label='EPS=0.01')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분산이 0인 환경: 결정적 환경(deterministic task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mus = [0.2, 0.8, 1.5, 0.5, 1.3, 0.5, -0.2, -1.0, 0.9, 0.4]\n",
    "mus = {'q'+str(i+1):mus[i] for i in range(len(mus))}\n",
    "sigmas = {'q'+str(i+1):0.0 for i in range(len(mus))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'q'+str(i+1):[] for i in range(len(mus))}\n",
    "for i in range(len(mus)):\n",
    "    for _ in range(1000):\n",
    "        key = 'q'+str(i+1)\n",
    "        data[key].append(np.random.normal(mus[key], sigmas[key]))\n",
    "data_pd = pd.DataFrame(data)\n",
    "sns.violinplot(data=data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt_00 = DobakAgent(0.0)\n",
    "agt_01 = DobakAgent(0.1)\n",
    "agt_001 = DobakAgent(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cavg_agt_00, cavg_agt_01, cavg_agt_001 = experiment(agt_00, agt_01, agt_001, sig=0)\n",
    "\n",
    "plt.plot(cavg_agt_00, color='r', label='Greedy')\n",
    "plt.plot(cavg_agt_01, color='g', label='EPS=0.1')\n",
    "plt.plot(cavg_agt_001, color='b', label='EPS=0.01')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경이 중간에 변화하는 경우라면(non-stationary task)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_n = 250\n",
    "\n",
    "def experiment_change(agt_00, agt_01, agt_001, sig=1, steps=500, n_runs=1000):\n",
    "    res_agt_00 = {}\n",
    "    res_agt_01 = {}\n",
    "    res_agt_001 = {}\n",
    "    for n in range(n_runs):\n",
    "        # 한 번의 런마다 하나의 문제를 생성한다.\n",
    "        mus = np.random.uniform(-3, 3, 10)\n",
    "        mus = {'q'+str(i+1):mus[i] for i in range(len(mus))}\n",
    "        sigmas = {'q'+str(i+1):sig for i in range(len(mus))}\n",
    "        res_agt_00[n] = []\n",
    "        res_agt_01[n] = []\n",
    "        res_agt_001[n] = []\n",
    "        # 에이전트들을 리셋 시킨다.\n",
    "        agt_00.reset()\n",
    "        agt_01.reset()\n",
    "        agt_001.reset()\n",
    "        for step in range(steps):\n",
    "            # 루프의 턴수가 250이 넘을 때마다 분포의 평균이 서로 바뀌는 상황\n",
    "            if (i+1)%change_n == 0:\n",
    "                vals = np.random.permutation(list(mus.values()))\n",
    "                for idx, k in enumerate(mus):\n",
    "                    mus[k] = vals[idx]\n",
    "            # 각 에이전트는 슬롯머신 손잡이를 선택한다.\n",
    "            action_00 = agt_00.decide()\n",
    "            action_01 = agt_01.decide()\n",
    "            action_001 = agt_001.decide()\n",
    "            # 각 에이전트의 선택에 따른 결과를 받는다.\n",
    "            reward_00 = np.random.normal(mus[action_00], sigmas[action_00])\n",
    "            reward_01 = np.random.normal(mus[action_01], sigmas[action_01])\n",
    "            reward_001 = np.random.normal(mus[action_001], sigmas[action_001])\n",
    "            res_agt_00[n].append(reward_00)\n",
    "            res_agt_01[n].append(reward_01)\n",
    "            res_agt_001[n].append(reward_001)\n",
    "            # 각 에이전트의 카운터를 올리고, 평균 보상값을 업데이트한다.\n",
    "            agt_00.N[action_00] += 1\n",
    "            agt_01.N[action_01] += 1\n",
    "            agt_001.N[action_001] += 1\n",
    "            agt_00.Q[action_00] += (1/agt_00.N[action_00]) * (reward_00 - agt_00.Q[action_00])\n",
    "            agt_01.Q[action_01] += (1/agt_01.N[action_01]) * (reward_01 - agt_01.Q[action_01])\n",
    "            agt_001.Q[action_001] += (1/agt_001.N[action_001]) * (reward_001 - agt_001.Q[action_001])\n",
    "    # 각 타입 스탭마다 2000번의 시행결과를 평균낸다.\n",
    "    res_00, res_01, res_001 = [], [], []\n",
    "    for step in range(steps):\n",
    "        temp_00, temp_01, temp_001 = [], [], []\n",
    "        for n in range(n_runs):\n",
    "            temp_00.append(res_agt_00[n][step])\n",
    "            temp_01.append(res_agt_01[n][step])\n",
    "            temp_001.append(res_agt_001[n][step])\n",
    "        res_00.append(np.mean(temp_00))\n",
    "        res_01.append(np.mean(temp_01))\n",
    "        res_001.append(np.mean(temp_001))\n",
    "    return res_00, res_01, res_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agt_00 = DobakAgent(0.0)\n",
    "agt_01 = DobakAgent(0.1)\n",
    "agt_001 = DobakAgent(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cavg_agt_00, cavg_agt_01, cavg_agt_001 = experiment_change(agt_00, agt_01, agt_001)\n",
    "\n",
    "plt.plot(cavg_agt_00, color='r', label='Greedy')\n",
    "plt.plot(cavg_agt_01, color='g', label='EPS=0.1')\n",
    "plt.plot(cavg_agt_001, color='b', label='EPS=0.01')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 왜 이런 탐색 방법이 필요한가?\n",
    "* 어떤 것을 선택하는 것이 좋은지를 먼저 알아야 한다.\n",
    "* 어떤 선택지가 더 나은 선택지인지를 알기 위해선 검증과정이 필요하다.\n",
    "* 실제 세계는 항상 변화하기에 지속적인 탐색이 필요하다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
