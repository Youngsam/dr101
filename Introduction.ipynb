{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥리워드 101: 강화학습의 기초개념 소개\n",
    "\n",
    "<br/>일시: 7월 20일 (토) 오후 1시 - 5시\n",
    "<br/>장소: 11 Terrace\n",
    "<br/>세션 1 - RL 기본 알고리즘 - DP, MC, TD / 발표자: 김영삼\n",
    "<br/>코드: [https://github.com/Youngsam/dr101](https://github.com/Youngsam/dr101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.researchgate.net/profile/Yaakov_Engel/publication/268412431/figure/fig2/AS:669401992945674@1536609280420/1-The-agent-environment-interaction-in-reinforcement-learning-10.png\" width=500 height=300>\n",
    "</p>\n",
    "<br/>\n",
    "\n",
    "## 1. 강화학습의 본질은 무엇인가?\n",
    "### 강화학습이란 \"합목적적 상호작용을 통한 학습\" 을 가리킨다.\n",
    "* 합목적적이란? $\\rightarrow$ **보상(reward)**을 **최대화**시키는 방향으로서\n",
    "* 상호작용이란? $\\rightarrow$ 학습의 주체, 즉 **에이전트**가 **환경**과 **정보**를 교환하는 양상\n",
    "* 여기서 정보란? $\\rightarrow$ 환경이 에이전트에게 주는 정보(**상태정보와 보상신호**)와 에이전트가 환경에게 제공하는 정보 혹은 가하는 **행동**\n",
    "\n",
    "### 강화학습은 환경에 대한 탐색을 이용한다는 점에서 지도학습과 다르다.\n",
    "* 지도학습(supervised learning)에서 에이전트는 지도의 대상이지 어떤 능동적 행동을 하는 존재가 아니다.\n",
    "* 지도학습에서 학습의 주목표는 지도된 목적과 에이전트가 받은 정보와의 연관성을 **활용(exploitation)**하는 것이다.\n",
    "* 때문에 지도학습에서 에이전트는 **탐색(exploration)**을 할 자유가 주어지지 않는다.\n",
    "\n",
    "### 강화학습은 환경이 주는 피드백에 의존한다는 점에서 비지도 학습과도 다르다.\n",
    "* 비지도 학습의 주목표는 주어진 데이터의 내적 구조를 **발견**하는 것이다.\n",
    "* 탐색적 과정이 비지도 학습에도 필요하나, 강화학습과는 달리 외부의 피드백 신호를 필요로 하지 않는다.\n",
    "* 때문에 비지도 학습은 환경으로부터 정보를 받기만 하지 주지는 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 강화학습에는 무엇이 중요한가?\n",
    "### 합목적적 행동을 위해서는 탐색이 필요하다.\n",
    "* 충분하고도 지속적인 탐색이 없이는 변화하는 환경에 성공적으로 적응할 수 없다.\n",
    "* 탐색을 통해 환경을 먼저 파악할 수 있어야 한다.\n",
    "\n",
    "### 탐색의 결과를 활용할 수 있어야 한다.\n",
    "* 탐색을 통해 환경에 대해 얻은 정보를 토대로 합목적적으로 행동할 수 있어야 한다.\n",
    "* 즉, 에이전트는 현재 환경에서 어떤 행동을 해야 보상이 증가하는 지를 예측해야 한다.\n",
    "\n",
    "### 탐색과 활용의 중도(中道)를 찾아야 한다.\n",
    "* 과도한 탐색은 이익실현의 기회를 낭비한다.\n",
    "* 탐색이 부족한 활용은 더 큰 이익획득의 기회를 놓치게 한다.\n",
    "* 이것을 탐색과 활용의 딜레마라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 강화학습은 왜 중요한가?\n",
    "* 불명확한 환경과 상호작용을 잘할 수 있는 시스템에 필요한 방법론이다.\n",
    "* 우리가 정답을 잘 알지 못하는 문제에 적용할 수 있는 학습 방법론이다.\n",
    "* 강화학습은 지적/인지적으로 행동하는 시스템에 대한 통합적 방법론을 제공한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 이 파트의 내용\n",
    "### I. 강화학습에서의 기본적 탐색법\n",
    "* [엡실론-그리디 방법](./N_Bandits/N_Bandits.ipynb)\n",
    "\n",
    "### II. 강화학습의 기본 프레임워크 \n",
    "* [마르코프 결정 과정과 다이내믹 프로그래밍](./Gridworld/MDP_DP.ipynb)\n",
    "\n",
    "### III. 강화학습의 기본 방법들\n",
    "* [몬테카를로 방법(Monte-Carlo Learning)](./Blackjack/MC_intro.ipynb)\n",
    "* [시간차 학습 방법(Temporal-Difference Learning)](./TD/TD_Learning.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
